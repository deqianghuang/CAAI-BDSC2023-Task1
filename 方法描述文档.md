# 方法描述文档

## 数据预处理

根据比赛提供的数据，我们进行了预处理。

初赛阶段我们统计了所有的user和event创建词典，之后对给的训练数据进行修改，将(inviter_id, event_id, voter_id)生成三元组作为训练数据，同时对于给定的target_event_preliminary_train_info.json文件中的元组，抽出1600条作为验证集，其余的加入训练集，最后生成训练集和验证集三元组文件。

复赛阶段对于新的target_event_final_train_info.json，我们抽取其中的800条作为验证集，其余的加入训练集，训练集中包含初赛和复赛的三元组数据共同进行训练，并在新的验证集上验证。

## 模型架构

我们主要采用了KGE的方式进行推理，在尝试了一系列的KGE模型之后选择了HAKE和RotatE作为编码的模型，我们在这两个模型上单独进行训练，之后利用两个模型集成推理生成结果。

### RotatE

RotatE是由论文《RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space》提出的KGE模型，该方法可以建模和推断各种关系模式，包括对称/反对称、逆反、组合。主要是通过将关系认定为在复矢量空间中从头实体到尾实体的旋转。

该模型启发于欧拉分解$e^{i \theta}=\cos \theta+i \sin \theta$，欧拉分解说明任何一个复数都可以看作一个复平面上的旋转向量。RotatE将实体和关系映射到复数向量空间，将每个关系定义为从头实体到尾实体的旋转。给定三元组$(h,r,t)$，期望$t=h \circ r, h, r, t \in C^k$，关系的模长为1，$\circ$表示Hadamard积。

经过数学推理和证明，该方法可以对三种关系进行有效的建模，如果关系为对称关系，可以得到Hadamard积为1，如果关系之间是逆反的，那就会得到向量表示互为逆，而如果关系是组合的，那就可以得到相加的关系。

同时模型在训练时提出了自对抗负采样，根据嵌入模型对负三元组进行采样，再根据loss进行训练$L=-\log \sigma\left(\gamma-d_r(h, t)\right)-\sum_{i=1}^n p\left(h_i^{\prime}, r, t_i^{\prime}\right) \log \sigma\left(d_r\left(h_i^{\prime}, t_i^{\prime}\right)-\gamma\right)$。

由于RotatE可以很好的建模多种类型的关系，我们将其作为嵌入的模型之一。

### HAKE

HAKE是由论文《Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction》提出的KGE模型，引入极坐标系针对目前的模型普遍没有利用的语义层级关系进行建模。

HAKE之前的模型依赖于对不同关系之间的连接模式建模，例如对称/反对称、可逆、组合等，但是都不能对语义层级进行建模，但是知识图谱中存在语义的层级，要想利用语义层级一般需要加入额外的信息如文本描述，但是HAKE可以在不增加额外的信息下，建模语义层级。

HAKE把实体嵌入分为两部分，不同的语义层级下的实体，使用极坐标的模长/极径(modulus)表示，具有更高语义层级的实体具有更小的模长；而同一语义层级下的不同实体，使用极坐标的相位/极角(phase)表示，不同的实体具有不同的极角。

<img src="https://s2.loli.net/2023/06/26/apwA9LIzx5MPdDt.png" alt="image-20230626174942968.png" style="zoom:20%;" />

给定实体$e$ 和关系 $r$，$e_m$和$r_m$表示实体和关系在modulus部分的编码，$e_p$和$r_p$表示实体和关系在phase部分的编码，给定$(h,r,t)$三元组，定义他们在modulus的距离为$d_{r, m}\left(\mathbf{h}_m, \mathbf{t}_m\right)=\left\|\mathbf{h}_m \circ \mathbf{r}_m-\mathbf{t}_m\right\|_2$，目标是$\mathbf{h}_m \circ \mathbf{r}_m=\mathbf{t}_m$，即头实体和关系元素乘积之后与尾实体的差尽可能小，定义他们在phase的距离为$d_{r, p}\left(\mathbf{h}_p, \mathbf{t}_p\right)=\left\|\sin \left(\left(\mathbf{h}_p+\mathbf{r}_p-\mathbf{t}_p\right) / 2\right)\right\|_1$，目标是$\left(\mathbf{h}_p+\mathbf{r}_p\right) \bmod 2 \pi=\mathbf{t}_p$，即让头实体和关系的相位相加之后更接近于尾实体的相位。最后相应的距离函数为：
$$
d_r(\mathbf{h}, \mathbf{t})=d_{r, m}\left(\mathbf{h}_m, \mathbf{t}_m\right)+\lambda d_{r, p}\left(\mathbf{h}_p, \mathbf{t}_p\right)
$$
在训练阶段，采用带有自对比训练的负采样损失函数：
$$
L=  -\log \sigma\left(\gamma-d_r(\mathbf{h}, \mathbf{t})\right) -\sum_{i=1}^n p\left(h_i^{\prime}, r, t_i^{\prime}\right) \log \sigma\left(d_r\left(\mathbf{h}_i^{\prime}, \mathbf{t}_i^{\prime}\right)-\gamma\right)
$$
由于HAKE可以建模实体和关系之间的语义层级，与本比赛的数据集有一定相似之处，经过试验测试，我们最终选择了HAKE作为主要的实体关系嵌入模型。

### 集成推理

在获得了实体和关系基于HAKE和RotatE的编码之后，我们针对测试集进行了集成推理。

对测试集中的inviter_id和event_id，我们将其转换成对应的序号并保存成头实体和关系，之后将所有的候选实体作为尾实体计算分数，并保存为score.npy中。之后我们对每一个要使用的模型，将分数通过sigmoid之后再取平均，作为最后的分数。

之后对每个测试案例，我们选出没有出现在对应头实体和关系训练集中的尾实体中分数最高的五个实体，作为候选的尾实体集合输出并保存。

## 实验参数选择及调参  

为了让模型运行出最好的结果，我们对不同参数的模型进行了调试并在验证集上进行了验证，最后选出了最优模型为：
```
model_list = ["models/HAKE_221s","models/RotatE_221s","models/RotatE_141s","models/HAKE_141s","models/HAKE_141s","models/HAKE_221","models/HAKE_511"]
```
其中这些模型的具体参数都会在models/具体模型/config.json里找到
这样不仅可以融合不同模型之间的差异性， 还能融合相同模型，不同参数的结果，优中选优，得到最后结果




